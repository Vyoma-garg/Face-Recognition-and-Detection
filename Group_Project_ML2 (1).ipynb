{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Group_Project_ML2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_LQU3VKqOATo",
        "outputId": "ba7e6c15-88f9-4c6c-d597-0e954e4cdff4"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HvsZ05l2X2Lk"
      },
      "source": [
        "**Define function-helpers for data preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n2ifHfoHOARr"
      },
      "source": [
        "import os \n",
        "import glob \n",
        "\n",
        "from PIL import Image, ImageDraw\n",
        "\n",
        "\n",
        "def get_files(path='./', ext=('.png', '.jpeg', '.jpg')):\n",
        "    \"\"\" Get all image files \"\"\"\n",
        "    files = []\n",
        "    for e in ext:\n",
        "        files.extend(glob.glob(f'{path}/**/*{e}'))\n",
        "    files.sort(key=lambda p: (os.path.dirname(p), int(os.path.basename(p).split('.')[0])))\n",
        "    return files\n",
        "\n",
        "def to_rgb_and_save(path):\n",
        "    \"\"\" Some of the images may have RGBA mode \"\"\"\n",
        "    for p in path:\n",
        "        img = Image.open(p)\n",
        "        if img.mode != 'RGB':\n",
        "            img = img.convert('RGB') \n",
        "            img.save(p)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fedZOaDJX-j3"
      },
      "source": [
        "**Define image path**\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FDfRtYNAYMhk"
      },
      "source": [
        "ABS_PATH = '/content/drive/My Drive/Colab Notebooks/facenet/'\n",
        "DATA_PATH = os.path.join(ABS_PATH, 'data')\n",
        " \n",
        "TRAIN_DIR = os.path.join(DATA_PATH, 'train_images')\n",
        "TEST_DIR = os.path.join(DATA_PATH, 'test_images')\n",
        "\n",
        "ALIGNED_TRAIN_DIR = TRAIN_DIR + '_cropped'\n",
        "ALIGNED_TEST_DIR = TEST_DIR + '_cropped'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8csu2Gs-cx7O"
      },
      "source": [
        "**Preparing data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHA_VX-vOAP2",
        "outputId": "f19338df-dfc6-47d4-b964-334719eb41aa"
      },
      "source": [
        "from collections import Counter \n",
        "\n",
        "\n",
        "# 1. Get path for TRAIN_DIR/TEST_DIR\n",
        "trainF, testF = get_files(TRAIN_DIR), get_files(TEST_DIR)\n",
        "\n",
        "# prepare info for printing\n",
        "trainC, testC = Counter(map(os.path.dirname, trainF)), Counter(map(os.path.dirname, testF))\n",
        "train_total, train_text  = sum(trainC.values()), '\\n'.join([f'\\t- {os.path.basename(fp)} - {c}' for fp, c in trainC.items()])\n",
        "test_total, test_text  = sum(testC.values()), '\\n'.join([f'\\t- {os.path.basename(fp)} - {c}' for fp, c in testC.items()])\n",
        "\n",
        "print(f'Train files\\n\\tpath: {TRAIN_DIR}\\n\\ttotal number: {train_total}\\n{train_text}')\n",
        "print(f'Train files\\n\\tpath: {TEST_DIR}\\n\\ttotal number: {test_total}\\n{test_text}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train files\n",
            "\tpath: /content/drive/My Drive/Colab Notebooks/facenet/data/train_images\n",
            "\ttotal number: 0\n",
            "\n",
            "Train files\n",
            "\tpath: /content/drive/My Drive/Colab Notebooks/facenet/data/test_images\n",
            "\ttotal number: 0\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kf2WT3ubOANw",
        "outputId": "b85381e3-f8e4-4d92-9680-72e3e643676c"
      },
      "source": [
        "# 2. Convert all img to RGB \n",
        "to_rgb_and_save(trainF), to_rgb_and_save(testF)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(None, None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wJB44d5802TR"
      },
      "source": [
        "**Image plotter fuctions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AXtHrHF80rXl"
      },
      "source": [
        "from math import ceil \n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from mpl_toolkits.axes_grid1 import ImageGrid\n",
        "%matplotlib inline \n",
        "\n",
        "from matplotlib.patches import Ellipse\n",
        "\n",
        "\n",
        "def imshow(img, ax, title):  \n",
        "    ax.imshow(img)\n",
        "    if title:\n",
        "        el = Ellipse((2, -1), 0.5, 0.5)\n",
        "        ax.annotate(title, xy=(1, 0), xycoords='axes fraction', ha='right', va='bottom',\n",
        "                    bbox=dict(boxstyle=\"round\", fc=\"0.8\"), \n",
        "                    arrowprops=dict(arrowstyle=\"simple\", fc=\"0.6\", ec=\"none\", \n",
        "                                    patchB=el, connectionstyle=\"arc3, rad=0.3\"))\n",
        "    ax.set_xticks([]), ax.set_yticks([])\n",
        "\n",
        "def plot_gallery(images, ncols, nrows, titles=None, title='', figsize=None): \n",
        "    if figsize is None: \n",
        "        figsize = (18, ncols) if ncols < 10 else (18, 20)  \n",
        "    fig = plt.figure(figsize=figsize)\n",
        "    grid = ImageGrid(fig, 111, nrows_ncols=(nrows, ncols), axes_pad=0.02)\n",
        "\n",
        "    for i, ax in enumerate(grid): \n",
        "        if i == len(images): break \n",
        "        imshow(images[i], ax, titles[i] if titles is not None else '')\n",
        "\n",
        "    # there are some problems with suptitle alignment \n",
        "    y_title_pos = grid[0].get_position().get_points()[1][1] - 0.33 / (1 if nrows == 1 else nrows / 3)\n",
        "    plt.suptitle(title, y=y_title_pos, fontsize=12)\n",
        "\n",
        "def plot(paths=None, images=None, titles=None, axtitle=True, title='', to_size=(512, 512)): \n",
        "    \"\"\"\n",
        "    Plot image gallery by passing (paths, title) or (images, titles)\n",
        "    :param paths: list of image paths\n",
        "    :param images: list of (PIL.Image | np.array | torch.Tensor) objects \n",
        "    :param titles: list of image titles \n",
        "    :param bool axtitle: if paths is not None, then axtitle=True leads to use basedir name as titles \n",
        "    :param str title: gallery title   \n",
        "    :param to_size: image resizing size before plot, default (512, 512)\n",
        "    \"\"\"\n",
        "\n",
        "    if paths is not None and len(paths): \n",
        "        images = [Image.open(p).resize(to_size) for p in paths]\n",
        "\n",
        "        nrows = int(ceil(len(images) / 12)) # 12 images per row \n",
        "        ncols = 12 if nrows > 1 else len(images)\n",
        "\n",
        "        if axtitle: \n",
        "              titles = [os.path.dirname(p).split('/')[-1] for p in paths]\n",
        "\n",
        "        plot_gallery(images, ncols, nrows, titles, title)\n",
        "\n",
        "    elif images is not None and len(images): \n",
        "        if isinstance(images, list): \n",
        "            images = np.array(images)\n",
        "\n",
        "        nrows = int(ceil(len(images) / 12)) # 12 images per row \n",
        "        ncols = 12 if nrows > 1 else len(images)\n",
        "\n",
        "        # Rescale to [0., 1.]\n",
        "        if images[0].max() > 1: \n",
        "            images /= 255. \n",
        "\n",
        "        # if torch.Tensor change axes \n",
        "        if not isinstance(images, np.ndarray): \n",
        "            if images.size(1) == 3 or 1: \n",
        "                images = images.permute((0, 2, 3, 1))\n",
        "\n",
        "        plot_gallery(images, ncols, nrows, titles, title)\n",
        "\n",
        "\n",
        "    else: \n",
        "        raise LookupError('You didnt pass any path or image objects')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "id": "hh0AbAtHwDC6",
        "outputId": "7bfaf86d-abe1-4c9d-a10f-026aacd469d4"
      },
      "source": [
        "plot(paths=trainF, title='Train images')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LookupError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-540bbfd9bfed>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtitle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'Train images'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-7-31ae95d9b1ea>\u001b[0m in \u001b[0;36mplot\u001b[0;34m(paths, images, titles, axtitle, title, to_size)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mLookupError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'You didnt pass any path or image objects'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLookupError\u001b[0m: You didnt pass any path or image objects"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zvb78oCKZjBc"
      },
      "source": [
        "plot(paths=testF, title='Test images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9ZoaZ_y95zc"
      },
      "source": [
        "**Install facenet_pytorch with MTCNN detection and pretrained vggface-2 InceptionResnetV1**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qci3jkvT94s9"
      },
      "source": [
        "!pip install facenet-pytorch \n",
        "\n",
        "from facenet_pytorch import MTCNN, InceptionResnetV1, training, fixed_image_standardization"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zG78gab01ALj"
      },
      "source": [
        "**Function for cropping and saving images based on MTCNN detector**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5q_vAM0sN_74"
      },
      "source": [
        "import tqdm \n",
        "\n",
        "import torch\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "import shutil\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Running on device: {device}')\n",
        "\n",
        "\n",
        "def crop_face_and_save(path, new_path=None, model=MTCNN, transformer=None, params=None):\n",
        "    \"\"\"\n",
        "    Detect face on each image, crop them and save to \"new_path\"\n",
        "    :param str path: path with images will be passed to  datasets.ImageFolder\n",
        "    :param str new_path: path to locate new \"aligned\" images, if new_path is None \n",
        "                     then new_path will be path + \"_cropped\" \n",
        "    :param model: model to detect faces, default MTCNN  \n",
        "    :param transformer: transformer object will be passed to ImageFolder\n",
        "    :param params: parameters of MTCNN model   \n",
        "    \"\"\"\n",
        "    if not new_path: \n",
        "        new_path = path + '_cropped'\n",
        "\n",
        "    # in case new_path exists MTCNN model will raise error \n",
        "    if os.path.exists(new_path):\n",
        "        shutil.rmtree(new_path)\n",
        "\n",
        "    # it is default parameters for MTCNN \n",
        "    if not params:\n",
        "        params = {\n",
        "            'image_size': 160, 'margin': 0, \n",
        "            'min_face_size': 10, 'thresholds': [0.6, 0.7, 0.7],\n",
        "            'factor': 0.709, 'post_process': False, 'device': device\n",
        "            }\n",
        "    \n",
        "    model = model(**params)\n",
        "\n",
        "    if not transformer:\n",
        "        transformer = transforms.Lambda(\n",
        "            lambd=lambda x: x.resize((1280, 1280)) if (np.array(x) > 2000).all() else x\n",
        "        )\n",
        "    # for convenience we will use ImageFolder instead of getting Image objects by file paths  \n",
        "    dataset = datasets.ImageFolder(path, transform=transformer)\n",
        "    dataset.samples = [(p, p.replace(path, new_path)) for p, _ in dataset.samples]\n",
        "\n",
        "    # batch size 1 as long as we havent exact image size and MTCNN will raise an error\n",
        "    loader = DataLoader(dataset, batch_size=1, collate_fn=training.collate_pil)\n",
        "    for i, (x, y) in enumerate(tqdm.tqdm(loader)): \n",
        "        model(x, save_path=y)\n",
        "\n",
        "    # spare some memory \n",
        "    del model, loader, dataset "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8bDrYCGP09dP"
      },
      "source": [
        "**Detect/Crop faces and save \"new\" aligned images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QsAMOMvnN_u_"
      },
      "source": [
        "# 3. Crop train dataset faces and save aligned images \n",
        "print('\\t- Train data')\n",
        "crop_face_and_save(TRAIN_DIR, ALIGNED_TRAIN_DIR)\n",
        "#detect_failed_train_files = []\n",
        "# check if some imgs were missed by detector and failed to save \n",
        "train_files, train_aligned_files = get_files(TRAIN_DIR), get_files(ALIGNED_TRAIN_DIR)\n",
        "if len(train_files) != len(train_aligned_files): \n",
        "    files = set(map(lambda fp: os.path.relpath(fp, start=TRAIN_DIR), train_files))\n",
        "    aligned_files = set(map(lambda fp: os.path.relpath(fp, start=ALIGNED_TRAIN_DIR), train_aligned_files))\n",
        "    detect_failed_train_files = list(files - aligned_files)\n",
        "    print(f\"\\nfiles {len(aligned_files)}/{len(files)}: {', '.join(detect_failed_train_files)} were not saved\")\n",
        "\n",
        "# -------------                     -------------\n",
        "\n",
        "# Crop test dataset faces and save aligned images \n",
        "print('\\t- Test data')\n",
        "crop_face_and_save(TEST_DIR, ALIGNED_TEST_DIR)\n",
        "\n",
        "# check if some imgs were missed by detector and failed to save \n",
        "test_files, test_aligned_files = get_files(TEST_DIR), get_files(ALIGNED_TEST_DIR)\n",
        "if len(test_files) != len(test_aligned_files): \n",
        "    files = set(map(lambda fp: os.path.relpath(fp, start=TEST_DIR), test_files))\n",
        "    aligned_files = set(map(lambda fp: os.path.relpath(fp, start=ALIGNED_TEST_DIR), test_aligned_files))\n",
        "    detect_failed_test_files = list(files - aligned_files)\n",
        "    print(f\"\\nfiles {len(aligned_files)}/{len(files)}: {', '.join(detect_failed_train_files)} were not saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HtIBvkiOZ3m1"
      },
      "source": [
        "trainF = get_files(ALIGNED_TRAIN_DIR)\n",
        "plot(paths=trainF, title='Aligned train images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7QQgXn1SZ6p8"
      },
      "source": [
        "testF = get_files(ALIGNED_TEST_DIR)\n",
        "plot(paths=testF, title='Aligned test images')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzBzx4eZaCLw"
      },
      "source": [
        "# Plot failed to aligned train images\n",
        "#trainFailF = list(map(lambda fp: os.path.join(TRAIN_DIR, fp), detect_failed_train_files))\n",
        "#plot(paths=trainFailF)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P9hmwqfMGogD"
      },
      "source": [
        "**Install albumentations for augmentations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y7x67ph6GiGq"
      },
      "source": [
        "!pip install albumentations\n",
        "\n",
        "import albumentations as A"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L_3rbM-KG2Nv"
      },
      "source": [
        "**Transformer for data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8vZf4a3d4Nr1"
      },
      "source": [
        "from facenet_pytorch import fixed_image_standardization\n",
        "\n",
        "standard_transform = transforms.Compose([\n",
        "                                np.float32, \n",
        "                                transforms.ToTensor(),\n",
        "                                fixed_image_standardization\n",
        "])\n",
        "\n",
        "aug_mask = A.Compose([\n",
        "                   A.HorizontalFlip(p=0.5),\n",
        "                   A.VerticalFlip(p=0.15),\n",
        "                   A.RandomContrast(limit=0.5, p=0.4),\n",
        "                   A.Rotate(30, p=0.2),\n",
        "                   A.RandomSizedCrop((120, 120), 160, 160, p=0.4),\n",
        "                   A.OneOrOther(A.JpegCompression(p=0.2), A.Blur(p=0.2), p=0.66),\n",
        "                   A.OneOf([\n",
        "                            A.Rotate(45, p=0.3),\n",
        "                            A.ElasticTransform(sigma=20, alpha_affine=20, border_mode=0, p=0.2)\n",
        "                            ], p=0.5),\n",
        "                  A.HueSaturationValue(val_shift_limit=10, p=0.3)\n",
        "], p=1)\n",
        "\n",
        "transform = {\n",
        "    'train': transforms.Compose([\n",
        "                                 transforms.Lambda(lambd=lambda x: aug_mask(image=np.array(x))['image']),\n",
        "                                 standard_transform\n",
        "    ]),\n",
        "    'test': standard_transform\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-0OpgmMZKmnf"
      },
      "source": [
        "**DataLoader for train/test**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jDeMxXJ0PhMF"
      },
      "source": [
        "from torch.utils.data import SubsetRandomSampler\n",
        "\n",
        "\n",
        "b = 32\n",
        "\n",
        "# Original train images \n",
        "trainD = datasets.ImageFolder(ALIGNED_TRAIN_DIR, transform=standard_transform)\n",
        "# Augmented train images \n",
        "trainD_aug = datasets.ImageFolder(ALIGNED_TRAIN_DIR, transform=transform['train'])\n",
        "# Train Loader\n",
        "trainL = DataLoader(trainD, batch_size=b, num_workers=2) # x: torch.Size([batch_size, 3, 160, 160]), y: torch.Size([batch_size])\n",
        "trainL_aug = DataLoader(trainD_aug, batch_size=b, num_workers=2)\n",
        "\n",
        "# Original test images \n",
        "testD = datasets.ImageFolder(ALIGNED_TEST_DIR, transform=standard_transform)\n",
        "# Test Loader\n",
        "testL = DataLoader(testD, batch_size=b, num_workers=2)\n",
        "\n",
        "# Convert encoded labels to named claasses\n",
        "IDX_TO_CLASS = np.array(list(trainD.class_to_idx.keys()))\n",
        "CLASS_TO_IDX = dict(trainD.class_to_idx.items())\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIEuTbOuNaQ5"
      },
      "source": [
        "**Prepare model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvjDBZk_NdLC"
      },
      "source": [
        "from facenet_pytorch import InceptionResnetV1\n",
        "\n",
        "model = InceptionResnetV1(pretrained='vggface2', dropout_prob=0.5, device=device).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6EJgRLGvL5QU"
      },
      "source": [
        "**Function for embedding extraction**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-nRsJLKM81M"
      },
      "source": [
        "def fixed_denormalize(image): \n",
        "    \"\"\" Restandartize images to [0, 255]\"\"\"\n",
        "    return image * 128 + 127.5\n",
        "\n",
        "def getEmbeds(model, n, loader, imshow=False, n_img=5):\n",
        "    model.eval()\n",
        "    # images to display \n",
        "    images = []\n",
        "\n",
        "    embeds, labels = [], []\n",
        "    for n_i in tqdm.trange(n): \n",
        "        for i, (x, y) in enumerate(loader, 1): \n",
        "\n",
        "            # on each first batch get 'n_img' images  \n",
        "            if imshow and i == 1: \n",
        "                inds = np.random.choice(x.size(0), min(x.size(0), n_img))\n",
        "                images.append(fixed_denormalize(x[inds].data.cpu()).permute((0, 2, 3, 1)).numpy())\n",
        "\n",
        "            embed = model(x.to(device))\n",
        "            embed = embed.data.cpu().numpy()\n",
        "            embeds.append(embed), labels.extend(y.data.cpu().numpy())\n",
        "\n",
        "    if imshow: \n",
        "        plot(images=np.concatenate(images))\n",
        "\n",
        "    return np.concatenate(embeds), np.array(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B95ZRkfbR1-G"
      },
      "source": [
        "**Extract embeddings**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k8Pi4jL5X9kZ"
      },
      "source": [
        "# 3. Get embeddings \n",
        "# Train embeddings \n",
        "trainEmbeds, trainLabels = getEmbeds(model, 1, trainL, False)\n",
        "trainEmbeds_aug, trainLabels_aug = getEmbeds(model, 50, trainL_aug, imshow=True, n_img=3)\n",
        "\n",
        "trainEmbeds = np.concatenate([trainEmbeds, trainEmbeds_aug])\n",
        "trainLabels = np.concatenate([trainLabels, trainLabels_aug])\n",
        "\n",
        "# Test embeddings \n",
        "testEmbeds, testLabels = getEmbeds(model, 1, testL, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KAZk_3Xsa5jL"
      },
      "source": [
        "# 4. Save embeddings \n",
        "TRAIN_EMBEDS = os.path.join(DATA_PATH, 'trainEmbeds.npz')\n",
        "TEST_EMBEDS = os.path.join(DATA_PATH, 'testEmbeds.npz')\n",
        "\n",
        "np.savez(TRAIN_EMBEDS, x=trainEmbeds, y=trainLabels)\n",
        "np.savez(TEST_EMBEDS, x=testEmbeds, y=testLabels)\n",
        "\n",
        "# Load the saved embeddings to use them futher \n",
        "trainEmbeds, trainLabels = np.load(TRAIN_EMBEDS, allow_pickle=True).values()\n",
        "testEmbeds, testLabels = np.load(TEST_EMBEDS, allow_pickle=True).values()\n",
        "\n",
        "# Get named labels\n",
        "trainLabels, testLabels = IDX_TO_CLASS[trainLabels], IDX_TO_CLASS[testLabels]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-R0sBZlXPqt"
      },
      "source": [
        "**Function for embedding calculations**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OHqwdeJ_4N-m"
      },
      "source": [
        "from sklearn.metrics import pairwise_distances\n",
        "import pandas as pd\n",
        "import seaborn as sns \n",
        "sns.set()\n",
        "\n",
        "def getDist(x, metric='euclidean', index=None, columns=None):\n",
        "    dists = pairwise_distances(x, x, metric=metric)\n",
        "    return pd.DataFrame(dists, index=index, columns=columns)\n",
        "\n",
        "def heatmap(x, title='', cmap='Greens', linewidth=1):\n",
        "    plt.figure(figsize=(17, 12))\n",
        "    plt.title(title)\n",
        "    sns.heatmap(x, cmap=cmap, square=True)\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOh9PB7-c-ot"
      },
      "source": [
        "**Get distance matrix for each image embedding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xt7zLvHbeHb4"
      },
      "source": [
        "# Note 88 first images are original and 4247 are augmented\n",
        "# as long as to calculate (4335, 512) distance matrix is time consuming we get only distances of originals \n",
        "inds = range(10)\n",
        "\n",
        "# Train embeddings \n",
        "dists = getDist(trainEmbeds[inds], metric='euclidean', index=trainLabels[inds], columns=trainLabels[inds])\n",
        "heatmap(dists, 'euclidean distance')\n",
        "\n",
        "dists = getDist(trainEmbeds[inds], metric='cosine', index=trainLabels[inds], columns=trainLabels[inds])\n",
        "heatmap(dists, 'cosine distance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3enjpCqRSC6a"
      },
      "source": [
        "# Test embeddings \n",
        "dists = getDist(testEmbeds, metric='euclidean', index=testLabels, columns=testLabels)\n",
        "heatmap(dists, 'euclidean distance')\n",
        "\n",
        "dists = getDist(testEmbeds, metric='cosine', index=testLabels, columns=testLabels)\n",
        "heatmap(dists, 'cosine distance')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjR2STiBnChj"
      },
      "source": [
        "**Take a look at how TSNE & PCA methods clustered our images**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0WAuRmSGRamb"
      },
      "source": [
        "from sklearn.manifold import TSNE\n",
        "\n",
        "\n",
        "inds = range(88)\n",
        "X_tsne1 = TSNE(n_components=2, init='pca', random_state=33).fit_transform(trainEmbeds[inds])\n",
        "X_tsne2 = TSNE(n_components=2, init='random', random_state=33).fit_transform(trainEmbeds[inds])\n",
        "y = [CLASS_TO_IDX[label] for label in trainLabels[inds]]\n",
        "\n",
        "fig, ax = plt.subplots(1, 2, figsize=(12, 8))\n",
        "\n",
        "img = ax[0].scatter(X_tsne1[:, 0], X_tsne1[:, 1], c=y, alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "ax[1].scatter(X_tsne2[:, 0], X_tsne2[:, 1], c=y, alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "\n",
        "ax[0].set_title('TSNE with pca init')\n",
        "ax[1].set_title('TSNE with random init')\n",
        "plt.suptitle('Face embeddings')\n",
        "\n",
        "cbar = plt.colorbar(img, ax=ax)\n",
        "cbar.ax.set_yticklabels(np.unique(trainLabels[inds])) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QQgvEV9fdnY"
      },
      "source": [
        "from sklearn.decomposition import PCA\n",
        "\n",
        "\n",
        "inds = range(88)\n",
        "X_pca1 = PCA(n_components=2, random_state=33).fit_transform(trainEmbeds[inds])\n",
        "y = [CLASS_TO_IDX[label] for label in trainLabels[inds]]\n",
        "\n",
        "fig, ax = plt.subplots(1, 1, figsize=(12, 8))\n",
        "img = ax.scatter(X_pca1[:, 0], X_pca1[:, 1], c=y, alpha=0.5, cmap=plt.cm.get_cmap('nipy_spectral', 10))\n",
        "\n",
        "plt.title('PCA method')\n",
        "plt.suptitle('Face embeddings')\n",
        "\n",
        "cbar = plt.colorbar(img, ax=ax)\n",
        "cbar.ax.set_yticklabels(np.unique(trainLabels[inds])) \n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h1hlJGrSsDyh"
      },
      "source": [
        "**Find optimal parameters for SVC classifier and train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aZXltW5IoZiZ"
      },
      "source": [
        "# data preparation \n",
        "X = np.copy(trainEmbeds)\n",
        "y = np.array([CLASS_TO_IDX[label] for label in trainLabels])\n",
        "\n",
        "print(f'X train embeds size: {X.shape}')\n",
        "print(f'Tagret train size: {y.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_USfruFsoI6"
      },
      "source": [
        "**As we see, in order to find optimal parameters among ```'param_grid'``` the whole search process took ~ 1.5 hours**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FpByRDQ2rLda"
      },
      "source": [
        "%%time \n",
        "\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n",
        "import warnings\n",
        "\n",
        "warnings.filterwarnings('ignore', 'Solver terminated early.*')\n",
        "\n",
        "param_grid = {'C': [1, 10, 100, 1e3, 5e3, 1e4, 5e4, 1e5],\n",
        "              'gamma': [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 'auto'],\n",
        "              'kernel': ['rbf', 'sigmoid', 'poly']}\n",
        "model_params = {'class_weight': 'balanced', 'max_iter': 10, 'probability': True, 'random_state': 3}\n",
        "model = SVC(**model_params)\n",
        "clf = GridSearchCV(model, param_grid)\n",
        "clf.fit(X, y)\n",
        "\n",
        "print('Best estimator: ', clf.best_estimator_)\n",
        "print('Best params: ', clf.best_params_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxFLzAG3uqj7"
      },
      "source": [
        "**Load & save SVC model, basically weights**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPCLqRJeW0PC"
      },
      "source": [
        "!pip install joblib\r\n",
        "import joblib as joblib"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ADvLChrkpHNV"
      },
      "source": [
        "\n",
        "SVM_PATH = os.path.join(DATA_PATH, 'svm.sav')\n",
        "joblib.dump(clf, SVM_PATH)\n",
        "clf = joblib.load(SVM_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7F4v97rKu_Y8"
      },
      "source": [
        "**Check the accuracy score on Train & Test datasets(embeddings)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6AhTXbedtbIB"
      },
      "source": [
        "# test data preparation \n",
        "X_test, y_test = np.copy(testEmbeds), np.array([CLASS_TO_IDX[label] for label in testLabels])\n",
        "print(f'X train embeds size: {X_test.shape}')\n",
        "print(f'Tagret train size: {y_test.shape}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwOt-SZ2vUCu"
      },
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "inds = range(88)\n",
        "train_acc = accuracy_score(clf.predict(X[inds]), y[inds])\n",
        "print(f'Accuracy score on train data: {train_acc:.3f}')\n",
        "\n",
        "test_acc = accuracy_score(clf.predict(X_test), y_test)\n",
        "print(f'Accuracy score on test data: {test_acc}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LCjl65qN1jlJ"
      },
      "source": [
        "**Some functional for video preprocessing**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dNBM-rg17Dql"
      },
      "source": [
        "import imageio\n",
        "\n",
        "def toGif(path, dim): \n",
        "    gpath = ''.join(path.split('.')[:-1]) + '.gif' \n",
        "\n",
        "    with imageio.get_writer(gpath, mode='I') as writer:\n",
        "        frames = []\n",
        "        capture = cv2.VideoCapture(path)\n",
        "    \n",
        "        i = 0 \n",
        "        while True: \n",
        "            ret, frame = capture.read()\n",
        "            if not ret: break \n",
        "\n",
        "            image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "            writer.append_data(cv2.resize(image, dim))\n",
        "            i += 1\n",
        "        print(f'Total frames: {i}')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "11RGIySdLMSC"
      },
      "source": [
        "# Create gifs from the existing videos in our path\n",
        "import cv2\n",
        "VIDEO_PATH = os.path.join(DATA_PATH, 'videos/')\n",
        "width, height = 640, 360\n",
        "\n",
        "mov1 = os.path.join(VIDEO_PATH, '1.mp4')\n",
        "toGif(mov1, (width, height))\n",
        "\n",
        "mov2 = os.path.join(VIDEO_PATH, '2.mp4')\n",
        "toGif(mov2, (width, height))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbcI16NbOXYd"
      },
      "source": [
        "**Bounding boxes can overlap, so define some function to avoid duplicates**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "atUqnp3lOU7j"
      },
      "source": [
        "def diag(x1, y1, x2, y2):\n",
        "    return np.linalg.norm([x2 - x1, y2 - y1])\n",
        "\n",
        "\n",
        "def square(x1, y1, x2, y2):\n",
        "    return abs(x2 - x1) * abs(y2 - y1)\n",
        "\n",
        "\n",
        "def isOverlap(rect1, rect2):\n",
        "    x1, x2 = rect1[0], rect1[2]\n",
        "    y1, y2 = rect1[1], rect1[3]\n",
        "\n",
        "    x1_, x2_ = rect2[0], rect2[2]\n",
        "    y1_, y2_ = rect2[1], rect2[3]\n",
        "\n",
        "    if x1 > x2_ or x2 < x1_: return False \n",
        "    if y1 > y2_ or y2 < y1_: return False\n",
        "  \n",
        "    rght, lft = x1 < x1_ < x2, x1_ < x1 < x2_\n",
        "    d1, d2 = 0, diag(x1_, y1_, x2_, y2_)\n",
        "    threshold = 0.5\n",
        "\n",
        "    if rght and y1 < y1_: d1 = diag(x1_, y1_, x2, y2)\n",
        "    elif rght and y1 > y1_: d1 = diag(x1_, y2_, x2, y1)\n",
        "    elif lft and y1 < y1_: d1 = diag(x2_, y1_, x1, y2) \n",
        "    elif lft and y1 > y1_: d1 = diag(x2_, y2_, x1, y1)\n",
        "\n",
        "    if d1 / d2 >= threshold and square(x1, y1, x2, y2) < square(x1_, y1_, x2_, y2_): return True\n",
        "    return False\n",
        "\n",
        "def draw_box(draw, boxes, names, probs, min_p=0.89):\n",
        "    font = ImageFont.truetype(os.path.join(ABS_PATH, 'arial.ttf'), size=22)\n",
        "\n",
        "    not_overlap_inds = []\n",
        "    for i in range(len(boxes)): \n",
        "        not_overlap = True\n",
        "        for box2 in boxes: \n",
        "            if np.all(boxes[i] == box2): continue \n",
        "            not_overlap = not isOverlap(boxes[i], box2)   \n",
        "            if not not_overlap: break \n",
        "        if not_overlap: not_overlap_inds.append(i)\n",
        "\n",
        "    boxes = [boxes[i] for i in not_overlap_inds] \n",
        "    probs = [probs[i] for i in not_overlap_inds]\n",
        "    for box, name, prob in zip(boxes, names, probs):\n",
        "        if prob >= min_p: \n",
        "            draw.rectangle(box.tolist(), outline=(255, 255, 255), width=5)\n",
        "            x1, y1, _, _ = box\n",
        "            text_width, text_height = font.getsize(f'{name}')\n",
        "            draw.rectangle(((x1, y1 - text_height), (x1 + text_width, y1)), fill='white')\n",
        "            draw.text((x1, y1 - text_height), f'{name}: {prob:.2f}', (24, 12, 30), font) \n",
        "            \n",
        "    return boxes, probs "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vnDLK-hxOoP5"
      },
      "source": [
        "**Functional for processing video**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DMxoNLxL6boe"
      },
      "source": [
        "standard_transform = transforms.Compose([\n",
        "                                transforms.Resize((160, 160)),\n",
        "                                np.float32, \n",
        "                                transforms.ToTensor(),\n",
        "                                fixed_image_standardization\n",
        "])\n",
        "\n",
        "def get_video_embedding(model, x): \n",
        "    embeds = model(x.to(device))\n",
        "    return embeds.detach().cpu().numpy()\n",
        "\n",
        "def face_extract(model, clf, frame, boxes):\n",
        "    names, prob = [], []\n",
        "    if len(boxes):\n",
        "        x = torch.stack([standard_transform(frame.crop(b)) for b in boxes])\n",
        "        embeds = get_video_embedding(model, x)\n",
        "        idx, prob = clf.predict(embeds), clf.predict_proba(embeds).max(axis=1)\n",
        "        names = [IDX_TO_CLASS[idx_] for idx_ in idx]\n",
        "    return names, prob \n",
        "\n",
        "def preprocess_image(detector, face_extractor, clf, path, transform=None):\n",
        "    if not transform: transform = lambda x: x.resize((1280, 1280)) if (np.array(x.size) > 2000).all() else x\n",
        "    capture = Image.open(path).convert('RGB')\n",
        "    i = 0 \n",
        "    \n",
        "    # iframe = Image.fromarray(transform(np.array(capture)))\n",
        "    iframe = transform(capture)\n",
        "   \n",
        "    boxes, probs = detector.detect(iframe)\n",
        "    if boxes is None: boxes, probs = [], []\n",
        "    names, prob = face_extract(face_extractor, clf, iframe, boxes)\n",
        "            \n",
        "    frame_draw = iframe.copy()\n",
        "    draw = ImageDraw.Draw(frame_draw)\n",
        "\n",
        "    boxes, probs = draw_box(draw, boxes, names, probs)\n",
        "    return frame_draw.resize((620, 480), Image.BILINEAR)\n",
        "\n",
        "\n",
        "def preprocess_video(detector, face_extractor, clf, path, transform=None, k=3):\n",
        "    frames = []\n",
        "    if not transform: transform = lambda x: x.resize((1280, 1280)) if (np.array(x.shape) > 2000).all() else x\n",
        "    capture = cv2.VideoCapture(path)\n",
        "    i = 0 \n",
        "    while True: \n",
        "        ret, frame = capture.read()\n",
        "        if not ret: break \n",
        "\n",
        "        iframe = Image.fromarray(transform(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))\n",
        "\n",
        "        if (i + 1) % k: \n",
        "            boxes, probs = detector.detect(iframe)\n",
        "            if boxes is None: boxes, probs = [], []\n",
        "            names, prob = face_extract(face_extractor, clf, iframe, boxes)\n",
        "            \n",
        "        frame_draw = iframe.copy()\n",
        "        draw = ImageDraw.Draw(frame_draw)\n",
        "\n",
        "        boxes, probs = draw_box(draw, boxes, names, probs)\n",
        "        frames.append(frame_draw.resize((620, 480), Image.BILINEAR))\n",
        "        i += 1\n",
        "        \n",
        "    print(f'Total frames: {i}')\n",
        "    return frames\n",
        "\n",
        "def framesToGif(frames, path):\n",
        "    with imageio.get_writer(path, mode='I') as writer:\n",
        "        for frame in tqdm.tqdm(frames): \n",
        "            writer.append_data(np.array(frame))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h45iPJFJlwqq"
      },
      "source": [
        "**Define our models and parameters**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "stp7QjVpXDy6"
      },
      "source": [
        "from PIL import ImageFont\n",
        "\n",
        "\n",
        "k = 3 # each k image will be processed by networks\n",
        "font = ImageFont.truetype(os.path.join(ABS_PATH, 'arial.ttf'), size=22)\n",
        "\n",
        "mtcnn = MTCNN(keep_all=True, min_face_size=70, device=device)\n",
        "model = InceptionResnetV1(pretrained='vggface2', dropout_prob=0.6, device=device).eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3yJgxDlhl417"
      },
      "source": [
        "**Process video and save to gif**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnMVKs8NgIIF"
      },
      "source": [
        "%%time \n",
        "print('Processing mov1: ')\n",
        "frames = preprocess_video(mtcnn, model, clf, mov1)\n",
        "mov1_aug = os.path.join(VIDEO_PATH, '1_aug.gif')\n",
        "framesToGif(frames, mov1_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VA_Y4QJakQHh"
      },
      "source": [
        "%%time\n",
        "print('Processing mov2: ')\n",
        "frames = preprocess_video(mtcnn, model, clf, mov2)\n",
        "mov2_aug = os.path.join(VIDEO_PATH, '2_aug.gif')\n",
        "framesToGif(frames, mov2_aug)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZZ2MZRq_kbWJ"
      },
      "source": [
        "ADD_DATA = os.path.join(DATA_PATH, 'images')\n",
        " \n",
        "frame = preprocess_image(mtcnn, model, clf, os.path.join(ADD_DATA, '1.jpg'))\n",
        "frame.save(os.path.join(ADD_DATA, '1_aug'), 'gif')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YVXc-_gnRZ-G"
      },
      "source": [
        "ADD_DATA = os.path.join(DATA_PATH, 'images')\n",
        " \n",
        "frame = preprocess_image(mtcnn, model, clf, os.path.join(ADD_DATA, '2.jpg'))\n",
        "frame.save(os.path.join(ADD_DATA, '2_aug'), 'gif')\n",
        "print(DATA_PATH)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHCAkYEW16VE"
      },
      "source": [
        "from IPython.display import HTML\r\n",
        "HTML('<img src=\"/content/drive/MyDrive/ColabNotebooks/facenet/data/videos/2_aug.gif\">')\r\n",
        "#   /content/drive/MyDrive/Colab Notebooks/facenet/data/videos/2_aug.gif"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sAXKnT3114U9"
      },
      "source": [
        "![VIDEO](/content/drive/MyDrive/ColabNotebooks/facenet/data/videos/2_aug.gif)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWvrdv6cClds"
      },
      "source": [
        "[Processed Video](/content/drive/MyDrive/ColabNotebooks/facenet/data/videos/2_aug.gif)\r\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IkDVjAuYE8LA"
      },
      "source": [
        ""
      ]
    }
  ]
}